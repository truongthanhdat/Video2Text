net: "./s2vt.prototxt"

# s2vt.prototxt supports multiple sequence to sequence architectures:
# (1) stage: 'factored' stage: '2-layer'
# (2) stage: 'unfactored' stage: '1-layer'
# (3) stage: 'unfactored' stage: '2-layer'
# Addons:
# (a) stage: 'dropFc7'   [input frame feature dropout]
# (b) stage: 'dropEn'    [text feature dropout after embedding]
# (c) stage: 'dropLstm1' [dropout on output of lstm1]
# (d) stage: 'dropLstm2' [dropout on output of lstm2]
#
# This solver uses variant (1) with dropouts which performed best on the movie corpora.
#
# To use a different variant, modify the states (train_state, test_state)
# below as appropriate:

train_state: { stage: 'factored' stage: '2-layer' stage: 'dropFc7' stage: 'dropEn' stage: 'dropLstm1' stage: 'dropLstm2'}
test_iter: 50
test_state: { stage: 'factored' stage: '2-layer' stage: 'test-on-train' }
test_iter: 50
test_state: { stage: 'factored' stage: '2-layer' stage: 'test-on-val' }
test_interval: 1000
iter_size: 1 # accumulate gradients to 1 batches before updating
base_lr: 0.0001
# Using adam solver starting with fixed base learning rate
lr_policy: "fixed"
gamma: 0.5
display: 1
max_iter: 65000
momentum: 0.9
momentum2: 0.999
weight_decay: 0.0000
snapshot: 10000
snapshot_prefix: "./snapshots/movies/mvad.vgg.adam0.0001_dropEn-dropFc7-dropLstm1-dropLstm2_factored_2layer_itersz1"
solver_mode: GPU
random_seed: 1701
average_loss: 100
clip_gradients: 10
solver_type: ADAM
